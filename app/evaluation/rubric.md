# RAG System Evaluation Rubric

## Overview

This rubric evaluates the quality of answers generated by the **Lecture Assistant RAG system** for the **Lecture 2** material (quadratic sorting algorithms, divide-and-conquer, binary search, merge sort, and matrix multiplication). Each answer is evaluated on four dimensions, each scored from 0 to 2, for a maximum total of **8 points per answer**.

* **Passing Threshold:** An answer is considered **acceptable** if it scores **≥ 6/8** (75%).
* **Project Goal:** At least **85%** of test questions (**≥ 13 out of 15**) should receive acceptable scores.

The rubric is designed to be applied to the 15-question evaluation set with categories:

* Definitions
* Complexity Analysis
* Algorithm Explanations
* Comparisons
* Pseudocode / Implementation

---

## Scoring Dimensions

### 1. Relevance (0–2 points)

**Does the answer directly address the question asked?**

* **0 points – Off-topic or Non-responsive**

  * Answer discusses unrelated topics (e.g., talks about hash tables when the question is about merge sort).
  * Completely ignores the specific focus of the question.
  * Provides generic information not clearly tied to the question.
  * **Example:**
    Q: “What is a divide-and-conquer algorithm?”
    A: “Algorithms are important in computer science because they solve problems efficiently.”

* **1 point – Partially Relevant**

  * Touches on the correct general topic but misses the core of what was asked.
  * Answers a related but different question (e.g., explains merge sort when the question is about the merge **operation**).
  * Provides only part of the requested information (e.g., describes bubble sort’s process but ignores the complexity asked for).
  * **Example:**
    Q: “What is the time complexity of binary search?”
    A: “Binary search looks at the middle element and then searches one half of the array.” (Mechanism explained, complexity omitted.)

* **2 points – Directly Relevant**

  * Directly addresses the main point of the question.
  * Focuses on what was **explicitly** asked (definition, complexity, pseudocode, comparison, etc.).
  * Stays on topic throughout the answer.
  * **Example:**
    Q: “What is the time complexity of binary search?”
    A: “The worst-case time complexity of binary search is O(log n), because at each step it halves the search interval.”

---

### 2. Correctness (0–2 points)

**Is the information factually accurate according to the lecture 2 material?**

* **0 points – Incorrect**

  * Contains major factual errors.
  * Contradicts the lecture (e.g., wrong complexity, wrong recurrence, wrong pseudocode structure).
  * Misstates the behavior of algorithms (e.g., says bubble sort always runs in O(n) time).
  * **Example:**
    “Merge sort has O(n²) time complexity” or
    “Strassen’s algorithm runs in O(n³) time.”

* **1 point – Mostly Correct with Minor Issues**

  * Core idea is correct but:

    * Some details are missing, slightly inaccurate, or imprecise.
    * Important conditions or qualifiers are omitted.
  * Small mistakes that do **not** fundamentally change the correctness of the answer.
  * **Examples:**

    * “Merge sort has O(n log n) time complexity” (correct but doesn’t mention that this holds in best, average, and worst cases).
    * “Binary search takes O(log n) steps” without stating the assumption that the array is sorted.

* **2 points – Fully Correct**

  * All stated facts are accurate and consistent with the lecture slides.
  * Includes important conditions and caveats when relevant (e.g., best vs worst case, in-place vs extra space).
  * Correctly states:

    * Time/space complexities.
    * Recurrence forms (e.g., T(n) = 7T(n/2) + O(n²)).
    * Algorithm steps and edge cases (e.g., base case in recursion).
  * **Example:**
    “Merge sort has time complexity O(n log n) in all cases (best, average, worst) because the array is recursively split into halves (log n levels) and each level does O(n) merging work.”

---

### 3. Use of Lecture Context & Citations (0–2 points)

**Does the answer demonstrate proper use of the lecture slides (retrieved chunks) and provide citations?**

* **0 points – No Context or Citations**

  * Answer reads like generic textbook or internet knowledge, with no clear link to the provided lecture.
  * No section names, page ranges, or chunk references.
  * Could have been written without using the RAG system at all.
  * **Example:**
    “Bubble sort is a simple algorithm that swaps out-of-order elements.” (No reference to where this appears in the slides.)

* **1 point – Weak Context / Citations**

  * Answer uses ideas from the lecture but:

    * Citations are vague or incomplete (e.g., just “[Lecture]”).
    * References are not clearly tied to specific claims.
    * Section or page references are present but not precise enough.
  * **Example:**
    “According to the lecture, binary search is efficient. [Binary Search slide]”

* **2 points – Strong Context & Citations**

  * Answer clearly grounded in specific lecture content:

    * Refers to section names (e.g., “Binary Search”, “Merge Sort Using Divide-and-Conquer”, “Matrix Multiplication – Strassen’s Algorithm”).
    * Uses page ranges or slide indices (e.g., “as shown on pages 27–30”).
  * Citations are aligned with the statements they support.
  * Demonstrates that the RAG system retrieved and used the correct lecture chunks.
  * **Example:**
    “Binary search has worst-case time complexity O(log n), because it repeatedly halves the search interval, as shown in the ‘Binary Search: Runtime Analysis’ slides on pages 30–34.”

---

### 4. Appropriate Detail & Clarity (0–2 points)

**Is the answer clear, structured, and at the right level of detail for the question?**

* **0 points – Poor Quality**

  * Too vague or too short to be useful.
  * Overly verbose with redundant or off-topic details.
  * Hard to follow due to poor structure or unclear language.
  * Omits critical steps (e.g., base case in recursive pseudocode).
  * **Example:**
    “It splits the thing and then merges stuff.” (For merge sort.)

* **1 point – Adequate but Flawed**

  * Understandable overall, but:

    * Too shallow for the question (e.g., no mention of recurrence when the question implies complexity reasoning).
    * Or unnecessarily detailed in irrelevant directions.
    * Organization is rough but still followable.
  * Some expected details from the **key_points** list are missing.
  * **Example:**
    Q: “Explain how recursive binary search works.”
    A: “Binary search checks the middle element and then goes left or right.”
    (Basic idea is there but missing base case, return of −1, and explicit recursive calls.)

* **2 points – Clear & Appropriate**

  * Well-structured, logically ordered explanation.
  * Uses simple, precise language suitable for a student.
  * Level of detail matches the question type:

    * Definition questions: concise but complete definitions.
    * Complexity questions: states complexity and briefly explains why.
    * Algorithm explanation: step-by-step, including base cases / loop structure where relevant.
    * Pseudocode: includes key lines and control structure.
  * **Example:**
    Q: “Explain how merge sort uses divide-and-conquer.”
    A: “Merge sort first checks if the array size is 1; if so, it returns. Otherwise, it splits the array into two halves, recursively sorts each half, and then merges the two sorted halves into a single sorted array, as shown in the ‘Merge Sort Using Divide-and-Conquer’ slides on pages 68–72.”

---

## Scoring Guidelines

### Total Score Interpretation

* **8 points:** Excellent – Perfect or near-perfect answer.
* **7 points:** Very Good – Minor improvements possible, but strong overall.
* **6 points:** Acceptable – Meets minimum standards (passing threshold).
* **5 points:** Below Standard – Significant issues in one or more dimensions.
* **4 points:** Poor – Multiple major problems, not reliable.
* **0–3 points:** Unacceptable – Fundamentally fails to answer correctly.

### Common Scoring Scenarios

**Scenario 1: Correct but Generic**

* Relevance: 2 (answers the question).
* Correctness: 2 (facts are correct).
* Use of Context: 0 (no reference to lecture slides).
* Detail & Clarity: 1 (somewhat brief or generic).
* **Total: 5/8 – FAIL** (Does not sufficiently use the RAG system’s lecture context.)

---

**Scenario 2: Well-Cited but Wrong**

* Relevance: 2 (on topic).
* Correctness: 0 (incorrect complexity or wrong pseudocode logic).
* Use of Context: 2 (good citations from lecture slides).
* Detail & Clarity: 1 (clear but fundamentally wrong).
* **Total: 5/8 – FAIL** (Accuracy is critical.)

---

**Scenario 3: Minimal but Correct**

* Relevance: 2 (directly answers question).
* Correctness: 2 (fully accurate).
* Use of Context: 1 (some reference to lecture, but vague).
* Detail & Clarity: 1 (on the short side, but understandable).
* **Total: 6/8 – PASS** (Meets threshold.)

---

**Scenario 4: Comprehensive Answer**

* Relevance: 2 (answers exactly what is asked).
* Correctness: 2 (all details match lecture).
* Use of Context: 2 (specific sections/pages/slide names).
* Detail & Clarity: 2 (clear, structured, right level of detail).
* **Total: 8/8 – EXCELLENT**

---

## Special Considerations

### For Pseudocode Questions (e.g., recursive binary search, merge sort, matrix multiplication)

* **Relevance:**

  * Must provide actual **step-by-step pseudocode** or a clearly structured sequence of steps that matches the lecture’s version, not just a vague description.
* **Correctness:**

  * Control flow and base cases must match the lecture slides (e.g., base case `left > right` in binary search, base case array size 1 in merge sort, correct quadrant multiplications in matrix multiplication).
* **Use of Context:**

  * Should reference where the pseudocode appears (e.g., “Recursive Binary Search Pseudocode”, “Merge Sort Using Divide-and-Conquer”, “Matrix Multiplication – Divide and Conquer / Strassen”).
* **Detail & Clarity:**

  * Should include key steps and structure:

    * Function signature / parameters.
    * Base case.
    * Recursive calls or loops.
    * Merge/combine logic or matrix block multiplications.

---

### For Complexity Analysis Questions (quadratic sorts, binary search, merge sort, matrix multiplication)

* **Relevance:**

  * Must directly address the requested complexity (time or space, and the specific case: best/average/worst).
* **Correctness:**

  * Big-O notation and reasoning must match the lecture:

    * Bubble/selection/insertion sorts: Θ(n²) (except best-case O(n) for bubble/insertion when noted).
    * Binary search: O(log n).
    * Merge sort: O(n log n) time, O(n) extra space.
    * Matrix multiplication recurrences:

      * Naive divide-and-conquer: T(n) = 8T(n/2) + O(n²) → O(n³).
      * Strassen: T(n) = 7T(n/2) + O(n²) → O(n^{log₂7}).
* **Use of Context:**

  * Should reference the relevant analysis slides (“Bubble Sort Analysis”, “Binary Search Runtime Analysis”, “Merge Sort Complexity”, “Matrix Multiplication and Strassen’s Algorithm”).
* **Detail & Clarity:**

  * Should briefly explain **why** the complexity holds (e.g., “two nested loops”, “halving search space”, “log n levels of recursion times O(n) work per level”).

---

### For Conceptual / Algorithm Explanation Questions

* **Relevance:**

  * Must explain the mechanism or paradigm being asked (e.g., divide-and-conquer, how merge operation works).
* **Correctness:**

  * Must align with the narrative and examples used in the lecture (e.g., divide, conquer, combine steps).
* **Use of Context:**

  * Should refer to the corresponding lecture sections, e.g., “Divide-and-Conquer Algorithms”, “Merge Operation”, “Quadratic Sorting Algorithms”.
* **Detail & Clarity:**

  * Include:

    * Key steps in order.
    * Base cases or termination conditions.
    * Intuition (when appropriate), such as “each pass bubbles the largest element to the right”.

---

### For Comparison Questions (sorting algorithms, recursive vs iterative binary search, naive vs Strassen)

* **Relevance:**

  * Must compare exactly the items requested (e.g., bubble vs insertion vs selection; recursive vs iterative binary search; naive matrix multiplication vs Strassen).
* **Correctness:**

  * Comparisons must be accurate:

    * Correct time/space complexities.
    * Correct trade-offs (e.g., simplicity vs performance, recursion overhead vs space).
* **Use of Context:**

  * Should reference cross-slides comparisons if present (e.g., “Quadratic Sorting Algorithms Summary”, “Recursive vs Iterative Binary Search”, “Matrix Multiplication – Strassen vs Divide-and-Conquer”).
* **Detail & Clarity:**

  * Cover key dimensions:

    * Complexity.
    * Best vs worst cases.
    * Memory usage.
    * Use cases (e.g., small vs large n, already sorted input).

---

## Evaluation Process

### Step 1: Prepare Test Set

* Use the defined 15-question evaluation JSON with categories:

  * Definitions (3)
  * Complexity Analysis (3)
  * Algorithm Explanations (3)
  * Comparisons (3)
  * Pseudocode (3)

### Step 2: Generate Answers

* For each question:

  * Run it through the Lecture Assistant RAG system.
  * Record:

    * The full generated answer.
    * The retrieved lecture chunks and their metadata (pages/sections).

### Step 3: Score Each Answer

* For each Q–A pair:

  * Read the question and the answer.
  * Cross-check with the lecture PDF (`lecture_2_sep_10_25.pdf`) as ground truth.
  * Assign scores (0–2) independently for:

    * Relevance
    * Correctness
    * Use of Lecture Context & Citations
    * Appropriate Detail & Clarity
  * Compute total score (0–8).
  * Mark as **PASS** if total ≥ 6, otherwise **FAIL**.

### Step 4: Analyze Results

* Compute:

  * Percentage of passing answers.
  * Average score per dimension across all questions.
* Identify:

  * Common failure modes (e.g., missing citations, shallow explanations, incorrect recurrences).
* Use this to:

  * Adjust prompts.
  * Improve retrieval or chunking.
  * Refine system behavior.

---

## Quality Checks

Before finalizing scores, ensure:

* ✅ You are using the **lecture PDF** as the ground-truth reference.
* ✅ Each dimension is scored **independently** according to its criteria.
* ✅ You are not “rounding up” scores to make results look better.
* ✅ Citations actually match the content on the cited pages/sections.
* ✅ Correctness is judged based on the **lecture slides**, not external sources.

---

## Example Evaluation

**Question:** “What is the worst-case time complexity of bubble sort and when does it occur?”

**Generated Answer (hypothetical):**
“Bubble sort has worst-case time complexity O(n²). This happens when the array is sorted in reverse order, so every pass requires many swaps. The algorithm must perform about n−1 passes and up to n−1 comparisons per pass, as shown in the ‘Bubble Sort Analysis’ slides on pages 6–8.”

**Scoring:**

* **Relevance:** 2/2

  * Directly answers about worst-case complexity and when it occurs.
* **Correctness:** 2/2

  * Complexity and explanation match the lecture.
* **Use of Lecture Context & Citations:** 2/2

  * Correct reference to “Bubble Sort Analysis” and pages 6–8.
* **Detail & Clarity:** 2/2

  * Clear explanation, appropriate level of detail.

**Total:** **8/8 – EXCELLENT**

---

## Rubric Maintenance

This rubric should be:

* ✅ Written and agreed **before** running evaluations.
* ✅ Applied consistently across all 15 questions.
* ✅ Not modified mid-evaluation (to avoid bias).
* ✅ Reviewed and possibly updated **between** evaluation cycles (e.g., after major system changes).
* ✅ Shared with all evaluators to ensure consistent scoring.
